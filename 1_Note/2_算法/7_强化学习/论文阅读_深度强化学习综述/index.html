<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读_深度强化学习综述 | Yan 的杂物志</title><meta name="author" content="Yan.xie"><meta name="copyright" content="Yan.xie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="journal: IEEE Conference on Industrial Electronics and Applications (ICIEA) name_ch: 深度强化学习：简要概述 name_en: A Brief Survey of Deep Reinforcement Learning date_publish: 2017 读后感 本篇介绍包含：应用场景；基于策略的和基于价值的不同">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读_深度强化学习综述">
<meta property="og:url" content="http://example.com/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="Yan 的杂物志">
<meta property="og:description" content="journal: IEEE Conference on Industrial Electronics and Applications (ICIEA) name_ch: 深度强化学习：简要概述 name_en: A Brief Survey of Deep Reinforcement Learning date_publish: 2017 读后感 本篇介绍包含：应用场景；基于策略的和基于价值的不同">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-01-03T00:00:00.000Z">
<meta property="article:modified_time" content="2023-09-03T10:08:42.136Z">
<meta property="article:author" content="Yan.xie">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读_深度强化学习综述',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-03 10:08:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="referrer" content="no-referrer"/><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">510</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">87</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Yan 的杂物志"><span class="site-name">Yan 的杂物志</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">论文阅读_深度强化学习综述</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-01-03T00:00:00.000Z" title="Created 2023-01-03 00:00:00">2023-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-09-03T10:08:42.136Z" title="Updated 2023-09-03 10:08:42">2023-09-03</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读_深度强化学习综述"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>journal: IEEE Conference on Industrial Electronics and Applications
(ICIEA)<br />
name_ch: 深度强化学习：简要概述<br />
name_en: A Brief Survey of Deep Reinforcement Learning<br />
date_publish: 2017</p>
<h2 id="读后感">读后感</h2>
<p>本篇介绍包含：应用场景；基于策略的和基于价值的不同方法；具体算法包含：DQN，TRPO，AC。<br />
这个文章，感觉中间那部分比较乱，结构比较乱，内容也只是皮毛，定义也没讲清楚，也不够深入，最后一部分还行。</p>
<h2 id="介绍">1. 介绍</h2>
<p>强化学习主要针对：自主的智能体通过与<strong>环境交互学习</strong>最优行为，通过<strong>试错随着时间的推移</strong>而改善模型。<br />
之前方法伸缩性差，且主要解释低维问题，深度学习算法近年的发展，借助其<strong>函数逼近</strong>和<strong>表示学习</strong>（高维状态和动作空间的降维&amp;分层表示）的能力，能更好解决上述问题。<br />
典型的应用场景，如从视频游戏的帧像素学习玩游戏，又如混合有监督学习和强化学习的AlphaGO（结合了监督，强化学习和传统的启发式搜索算法），还有机器人等等。</p>
<h2 id="奖利驱动行为">2. 奖利驱动行为</h2>
<p>原理：强化学习<strong>在交互环境中学习回报更高的行为</strong>（基于行为主义的试错法）。</p>
<p>常用表示：<br />
t: time step 时间步<br />
s: state 状态，有时也记作xt<br />
a: action 行为或动作，有时也记作ut<br />
r: reward 奖利<br />
<span class="math inline">\(\pi\)</span>: policy
目标策略，最大于奖利<br />
详见：<a
href="1_Note/2_算法/7_强化学习/1_强化学习_基础知识.md#4%20强化学习中的元素">强化学习中的元素</a></p>
<p>在时间步t，给定一个状态<span
class="math inline">\(s_t\)</span>，通过目前已知的所有信息计算最佳动作<span
class="math inline">\(a_t\)</span>，执行该动作后，策略返回一个奖利，状态变为st+1，如果往复，在不断试错学习过程中更新知识库。<br />
RL最大的挑战是需要通过试错学习一系列的动作（<strong>而非一个动作</strong>）。</p>
<p>每一次与环境的交互都会产生信息，智能体利用这些信息来更新自己的知识。这种感知-行为学习回路如图2所示。<br />
[[Pasted image 20230109181228.png|500]]</p>
<h3 id="a.-马尔可夫决策过程">A. 马尔可夫决策过程</h3>
<p><a
href="1_Note/2_算法/7_强化学习/1_强化学习_基础知识#3%20马尔可用决策过程">马尔可用决策过程</a></p>
<h3 id="b.-rl面临的挑战">B. RL面临的挑战</h3>
<ul>
<li>最优策略必须与环境的试错交互，它接收到学习信号只有奖励。<br />
</li>
<li>观察依赖行为，且有很强的时序相关性。<br />
</li>
<li>需要处理长期的时间依赖性：一个动作的结果通常只有在多次环境转换之后才会显现出来（是动作累积的结果），这被称为"时序信用分配问题"。<br />
比如：机器人在迷宫里走，它看到了什么取决了它选择怎么走，这也是探索/利用的问题，这些问题都可以在RL的框架中得到解决。</li>
</ul>
<h2 id="强化学习算法">3. 强化学习算法</h2>
<p>下面将讨论不同类型的强化学习算法。主要包含：基于价值函数的算法，基策略搜索的算法和混合二者的算法A-C。</p>
<h3 id="a.-价值函数">A. 价值函数</h3>
<p>详见： <a href="1_强化学习_基础知识#4.6%20价值%20Value">价值
Value</a></p>
<h4 id="动态规划">动态规划</h4>
<p>贝尔曼方程是动态规划算法的一种重要的组成部分。它通常用来描述一个问题的最优解是由之前状态的最优解转移而来的。<br />
详见：<a
href="4_强化学习_经典算法#1.2%20动态规划法（dynamic%20programming）">动态规划法</a></p>
<h3 id="b.-采样">B. 采样</h3>
<p>相对于动态规划逐步计算值函数，蒙特卡洛方法通过平均多次rollout（尝试探索一条路径）的收益进行来估计一个状态的预期收益。结合TD和蒙特卡罗可以发挥二者的优势。</p>
<p>另一种计算值函数的方法引入了优势函数A，用于表示某个策略不同与其它策略的价值，使用此方法可以对比不同行为的差异。<br />
<span class="math display">\[A^{\pi}=Q^{\pi}-V^{\pi}\]</span><br />
具体用法可参见：<a
href="5_强化学习_深度学习算法#1.2.2%20Dueling%20DQN">Dueling DQN</a></p>
<p>采样方法如图-3所示：<br />
[[Pasted image 20230109192854.png]]<br />
这里的BACKUP指的是数据，(a)是动态规划，(b)是穷举搜索，它们都使用所有数据计算策略；(c)是时序差分(bootstrap)，(d)是蒙特卡罗，它们使用对数据采样的方法。</p>
<h3 id="c.-搜索策略">C. 搜索策略</h3>
<p>策略搜索方法不需要维护价值函数，而是直接优化策略<span
class="math inline">\(\pi\)</span>，以获得最大的奖利E(R)，这种方法可以使用梯度或非梯度方法优化，使用神经网络实现。梯度方法尤其在参数维度高的情况下更加适用。它一般直接输出概率分布，然后利用概率对行为进行采样；神经网络也可支持无梯度策略，比如低维空间，它不需要策略是可微的。</p>
<h4 id="策略梯度">策略梯度</h4>
<p><a
href="1_Note/2_算法/7_强化学习/5_强化学习_深度学习算法#2%20基于策略的方法：INFORCE算法">基于策略的方法：INFORCE算法</a></p>
<h4 id="a-c方法">A-C方法</h4>
<p><a
href="1_Note/2_算法/7_强化学习/5_强化学习_深度学习算法.md#3%20整合价值和策略%20A-C">整合价值和策略
A-C</a><br />
后面将AC作为策略方法的子集介绍。</p>
<h3 id="d.-计划和学习">D. 计划和学习</h3>
<p>Sutton和Barto将规划定义为利用模型来产生或改进政策的任何方法。这包括分布模型(包括T和R)和样本模型(对状态转换关系采样)。</p>
<p>基于模型的RL方法可以用模型模拟状态转换，以提升采样效率，但是学习模型也可能带来误差，常用的解决方法是只在短序列（局部）使用模型。</p>
<h3 id="e.-使用深度学习方法带来的提升">E.
使用深度学习方法带来的提升</h3>
<p>一般RL方法只适用于低维空间，深度学习方法可将其扩展到高维空间（特征映射，低维表示）；另外，它可以处理像图像等更复杂的数据；更多时候用深度学习来优化策略，价值函数等。</p>
<p>其主要的优势在于梯度能
很好的引导学习。另外，深度学习可以同时优化多个目标，比如同时优化策略、模型、价值函数等，使其相互促进。另外，在rollouts的过程中错误会自动积累，这也避免也手动累加。</p>
<p>下面两章将主要介绍深度学习实现价值函数和策略中的方法。</p>
<h2 id="价值函数详解">4. 价值函数详解</h2>
<p><strong>最早从值函数开始</strong><br />
深度学习最早使用TD（时序差分）90年代在双陆棋中的应用。后来RL研究的发展倾向于使用价值函数，用以捕捉环境的底层结构。DRL中早期的值函数方法将简单的状态作为输入，现在的方法能够处理视觉上和概念上复杂的环境。</p>
<h3 id="a.-函数逼近和dqn">A. 函数逼近和DQN</h3>
<p>[[Pasted image 20230107115047.png]]<br />
DQN可以用于学习视频游戏的玩法，其输入是图片，使用CNN提取时空特征，最后再连接全连接层从而对行为决策。最早的出现的NFQ(neural
fitted Q)，可接收视频输入，降低数据维度，用一个单独分枝预测Q值。<br />
DQN可以接受环境中各种各样的输入，不仅可以让最后一层更好的选择动作，也让卷积层更好的学习与动作无关的表示，从而学到的显著的视觉特征。</p>
<p>如果不用深度学习，如果把每不同的帧作为状态（长x宽x色深），再考虑可能的行为，二项相乘得到Q将是非常大而稀疏的，且学到的一个状态-行为不能传递给附近的状态-行为。</p>
<p>深度学习使用了函数逼近方法解决上述问题，具体方法是：<br />
* 重放 replay<br />
重放机制将历史经历(st, at, st+1,
rt+1)存储在循环缓冲区中，并从中重新采样这些经验来更新策略或决策过程（离线）。这样，在训练过程中就可以使用大量的经验数据，提高模型的效率（批处理提升吞吐量）和泛化能力，并避免数据的冗余或不足。<br />
* 目标网络 target networks<br />
目标网络初始值是制定策略的网络权重，在开始的一般时间只更新Q值，而不更新目标网络，直到一定步数后才更新。<br />
详见：<a href="5_强化学习_深度学习算法#1.1%20DQN技术">DQN技术</a></p>
<h3 id="b.-修改q函数">B. 修改Q函数</h3>
<p>DQN的关键组成部分之一是Q函数的函数逼近器，它依赖于Q函数的实现和优化。本节列出了Q函数优化方法：<br />
Q学习中使用单一的评价，使用最大动作价值作为期望值因此高估的收益，<strong>Double-DQN</strong>学习需要学习一个额外的函数，实现了更好的评估。<br />
另一种方法是学习价值分布而不仅是价值期望。它提供了额外的信息，比如潜在的奖励是来自偏态分布还是多模态分布。<br />
还有一种方法是把Q函数分解成状态价值和优势(我觉得是动作价值)：Q=V+A，相对来说A更容易学习，<strong>对决DQN(dueling)</strong>
与有优先级的replay相结合是离散动作最先进的模型之一；Gu提出的凸性优势层将算法扩展到连续的动作空间，建立了标准化优势函数，结合replay,
目标网络主优势更新，是连续动作最先进的模型之一。<br />
一些领域有大量的离散行为，比如推荐系统，Dulac-Arnold提出“行为嵌入”，使用k近邻产生“原行为”以利用传统的RL方法，或者使用表示学习。<br />
另一个场景是同时决策多个行为，比如机器人动作，组合也会使action指数性增长。比较简单的方法是把复杂行为分解成简单行为分别处理；还有自回归的方法来处理多步预测，离散化大的行为空间；在更广泛的背景下，不直接处理原始行动，可以选择从更高层级的政策中调用"子策略"，称为分层强化学习(
HRL )。<br />
详见：<a
href="1_Note/2_算法/7_强化学习/5_强化学习_深度学习算法#1.2%20其它DQN">其它DQN</a></p>
<h2 id="策略搜索详解">5. 策略搜索详解</h2>
<p>策略搜索是直接寻找策略的方法，它可以是基于梯度，也可以是不基于梯度的。比如不基于剃度的进行算法，这种算法计算量大，但应用范围广，不像梯度方法只能作用于连续空间。也有一些将进化算法和神经网络压缩相结合的方法。</p>
<h3 id="a.-随机函数反向传播">A. 随机函数反向传播</h3>
<p>反向传播不仅能解决强化学习中具体的跟踪，图片分类等问题，而且利用反向传播能力优化参数，它也是SVGs[^1]等算法中的关键原理。</p>
<h3 id="b.-错误积累">B. 错误积累</h3>
<p>直接搜索参数多的神经网络可能很困难，并受到局部最小值的影响。下面介绍一些解决错误积累的方法：<a
href="1_Note/2_算法/7_强化学习/5_强化学习_深度学习算法#4%20其它优化">其它优化</a></p>
<h3 id="c.-a-c方法">C. A-C方法</h3>
<p>详见：<a
href="1_Note/2_算法/7_强化学习/5_强化学习_深度学习算法.md#3%20整合价值和策略%20A-C">整合价值和策略
A-C</a></p>
<h2 id="目前的研究和挑战">目前的研究和挑战</h2>
<p>详见：<a
href="1_Note/2_算法/7_强化学习/6_强化学习_进阶">强化学习_进阶</a></p>
<h2 id="总结超越模型识别">总结：超越模型识别</h2>
<p>除DRL以外，最近还有一些新发展，比如生成式因果模型（非深度学习）相对于A3C表现出了更优越的泛化能力。<br />
目前DRL已与搜索和规划相结果，与传统方法相比有更好的泛化性，可解释性，支持更复杂的环境，但有待进一步提升。<br />
RL能与周围环境互动，更贴近现实世界，让agent通过实验更好理解周围环境，学到更高层的因果关系。</p>
<h2 id="其它">其它</h2>
<ul>
<li><strong>SARSA算法</strong><br />
state-action-reward-state-action<br />
</li>
<li>动态规划<br />
动态规划的基本思想是：将待求解的问题分成若干个子问题，按顺序求解子问题，前面的子问题的解用于后面的子问题的求解，最终得出整个问题的解。</li>
</ul>
<h3 id="启发">启发</h3>
<ul>
<li>可不可以用深度学习把高维特征转到低维，可以的，输入视频学玩游戏就是这样。<br />
</li>
<li>在ICU里训练一个可以模拟环境的目标网络和决策网络同时训练有可能是可以的。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://example.com">Yan.xie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/">http://example.com/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/1_Note/0_%E5%B7%A5%E5%85%B7/%E7%AC%94%E8%AE%B0%E5%B7%A5%E5%85%B7/zotero/%E6%96%87%E7%8C%AE%E5%B7%A5%E5%85%B7_Zotero/" title="文献工具_Zotero"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">文献工具_Zotero</div></div></a></div><div class="next-post pull-right"><a href="/1_Note/2_%E7%AE%97%E6%B3%95/10_%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E5%8C%BB%E5%AD%A6%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/" title="医学异常检测综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">医学异常检测综述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/0_Inbox/0_%E6%AD%A3%E5%9C%A8%E7%9C%8B/3_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_InstructGPT/" title="3_论文阅读_InstructGPT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-08</div><div class="title">3_论文阅读_InstructGPT</div></div></a></div><div><a href="/0_Inbox/%E7%83%82%E5%B0%BE/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/" title="深度强化学习综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-27</div><div class="title">深度强化学习综述</div></div></a></div><div><a href="/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_AlphaGo_Zero/" title="论文阅读_AlphaGo_Zero"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-17</div><div class="title">论文阅读_AlphaGo_Zero</div></div></a></div><div><a href="/1_Note/2_%E7%AE%97%E6%B3%95/7_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96_PPO/" title="论文阅读_近端策略优化_PPO"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-04</div><div class="title">论文阅读_近端策略优化_PPO</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yan.xie</div><div class="author-info__description">一些想法</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">510</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">87</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%90%8E%E6%84%9F"><span class="toc-number">1.</span> <span class="toc-text">读后感</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%96%E5%88%A9%E9%A9%B1%E5%8A%A8%E8%A1%8C%E4%B8%BA"><span class="toc-number">3.</span> <span class="toc-text">2. 奖利驱动行为</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a.-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">A. 马尔可夫决策过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b.-rl%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">3.2.</span> <span class="toc-text">B. RL面临的挑战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">3. 强化学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a.-%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">A. 价值函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="toc-number">4.1.1.</span> <span class="toc-text">动态规划</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b.-%E9%87%87%E6%A0%B7"><span class="toc-number">4.2.</span> <span class="toc-text">B. 采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c.-%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5"><span class="toc-number">4.3.</span> <span class="toc-text">C. 搜索策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.3.1.</span> <span class="toc-text">策略梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#a-c%E6%96%B9%E6%B3%95"><span class="toc-number">4.3.2.</span> <span class="toc-text">A-C方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#d.-%E8%AE%A1%E5%88%92%E5%92%8C%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.4.</span> <span class="toc-text">D. 计划和学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#e.-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%8F%90%E5%8D%87"><span class="toc-number">4.5.</span> <span class="toc-text">E.
使用深度学习方法带来的提升</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3"><span class="toc-number">5.</span> <span class="toc-text">4. 价值函数详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a.-%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91%E5%92%8Cdqn"><span class="toc-number">5.1.</span> <span class="toc-text">A. 函数逼近和DQN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b.-%E4%BF%AE%E6%94%B9q%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">B. 修改Q函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%90%9C%E7%B4%A2%E8%AF%A6%E8%A7%A3"><span class="toc-number">6.</span> <span class="toc-text">5. 策略搜索详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a.-%E9%9A%8F%E6%9C%BA%E5%87%BD%E6%95%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.1.</span> <span class="toc-text">A. 随机函数反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#b.-%E9%94%99%E8%AF%AF%E7%A7%AF%E7%B4%AF"><span class="toc-number">6.2.</span> <span class="toc-text">B. 错误积累</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#c.-a-c%E6%96%B9%E6%B3%95"><span class="toc-number">6.3.</span> <span class="toc-text">C. A-C方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%89%8D%E7%9A%84%E7%A0%94%E7%A9%B6%E5%92%8C%E6%8C%91%E6%88%98"><span class="toc-number">7.</span> <span class="toc-text">目前的研究和挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E8%B6%85%E8%B6%8A%E6%A8%A1%E5%9E%8B%E8%AF%86%E5%88%AB"><span class="toc-number">8.</span> <span class="toc-text">总结：超越模型识别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E5%AE%83"><span class="toc-number">9.</span> <span class="toc-text">其它</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8F%91"><span class="toc-number">9.1.</span> <span class="toc-text">启发</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/1_Note/2_%E7%AE%97%E6%B3%95/5_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E5%85%B7/EasyNLP/" title="EasyNLP">EasyNLP</a><time datetime="2023-09-03T09:55:41.108Z" title="Created 2023-09-03 09:55:41">2023-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2_Knowledge/2_%E6%8A%80%E6%9C%AF/%E4%B8%BB%E9%A2%98%E7%AC%94%E8%AE%B0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="深度学习模型压缩">深度学习模型压缩</a><time datetime="2023-09-03T09:55:40.488Z" title="Created 2023-09-03 09:55:40">2023-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2_Knowledge/2_%E6%8A%80%E6%9C%AF/%E4%B8%BB%E9%A2%98%E7%AC%94%E8%AE%B0_%E5%A2%9E%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" title="增强语言模型导读">增强语言模型导读</a><time datetime="2023-09-02T00:00:00.000Z" title="Created 2023-09-02 00:00:00">2023-09-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/1_Note/0_%E5%B7%A5%E5%85%B7/ChatGPT/llamaindex_%E5%88%86%E4%BA%AB/" title="llamaindex_分享">llamaindex_分享</a><time datetime="2023-09-02T00:00:00.000Z" title="Created 2023-09-02 00:00:00">2023-09-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/1_Note/0_%E5%B7%A5%E5%85%B7/ChatGPT/%E8%AF%95%E7%94%A8MetaGPT/" title="试用MetaGPT">试用MetaGPT</a><time datetime="2023-09-02T00:00:00.000Z" title="Created 2023-09-02 00:00:00">2023-09-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Yan.xie</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>